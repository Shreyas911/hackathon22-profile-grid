{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data processing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "\n",
    "#data visualization\n",
    "%matplotlib inline\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data processing\n",
    "import requests, copy, math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.interpolate\n",
    "\n",
    "#data visualization\n",
    "import matplotlib.pylab as plt\n",
    "from matplotlib import ticker\n",
    "\n",
    "#used for map projections\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cft\n",
    "from cartopy.mpl.gridliner import LONGITUDE_FORMATTER, LATITUDE_FORMATTER\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "########### data processing #################\n",
    "#####\n",
    "# check if there is an error message\n",
    "def check_error_message(ans,writeFlag=False):\n",
    "    # ans: response JSON from an API query\n",
    "    # writeFlag: bool, true == print verbose errors, if found\n",
    "    # returns error code if found, or NaN if not.\n",
    "    if isinstance(ans,dict) and 'message' in ans.keys() and 'code' in ans.keys():\n",
    "        if writeFlag:\n",
    "            print(str(ans['code']) + ': ' + ans['message'])\n",
    "        ##### NOTE: we should include here below all the codes that do not return data as the user expects\n",
    "        if ans['code'] >= 400 and ans['code'] != 404:\n",
    "            print('Data were not returned')\n",
    "            print(ans)\n",
    "            raise Exception('No data')\n",
    "        return ans['code']        \n",
    "    elif ans:\n",
    "        return np.nan\n",
    "#####\n",
    "# check if the object is a list of dictionaries\n",
    "def check_list_of_dict(lst,writeFlag=False):\n",
    "    # lst: object to check if is a list of dicts\n",
    "    # writeFlag: bool, true == verbose mode\n",
    "    # return 1 if lst is a list of dicts, 0 ow\n",
    "    if lst and isinstance(lst,list):\n",
    "        if all(isinstance(i, dict) for i in lst):\n",
    "            if writeFlag:\n",
    "                print('Number of items: '+str(len(lst)))\n",
    "            return 1\n",
    "        else:\n",
    "            if writeFlag:\n",
    "                print(lst) \n",
    "            return 0\n",
    "    else:\n",
    "        if writeFlag:\n",
    "            print(lst) \n",
    "        return 0 \n",
    "######\n",
    "def get_data_from_url(url,myAPIkey,writeFlag=False):\n",
    "    # url: string url to attempt to query\n",
    "    # myAPIkey: string API key, get yours at https://argovis-apikey-manager-atoc-argovis-dev.apps.containers02.colorado.edu/ \n",
    "    # myAPIkey can also be left empty '', yet in this case the user is more likely to exceed API request limits (and get HTTP 403 errors).\n",
    "    # returns a dictionary representation of the reponse from the endpoint hit in url; empty list if 404.\n",
    "    try:\n",
    "        d_raw = requests.get(url,headers={\"x-argokey\": myAPIkey}).json()\n",
    "        ans = check_error_message(ans=d_raw,writeFlag=writeFlag)\n",
    "    except:\n",
    "        print(url)\n",
    "        raise Exception('No data')\n",
    "    # check that data are a list of dictionaries as expected\n",
    "    if ans == 404:\n",
    "        return []\n",
    "    elif np.isnan(ans) and check_list_of_dict(lst=d_raw,writeFlag=writeFlag) == 1:\n",
    "        if writeFlag:\n",
    "            print(url)\n",
    "        return d_raw\n",
    "    else:\n",
    "        print(ans)\n",
    "        raise Exception('Check object type and error code')\n",
    "#####\n",
    "def create_url(url_prefix, \\\n",
    "               startDate='',endDate='', \\\n",
    "               radius_km=[],center=[], \\\n",
    "               polygon=[],data='',presRange='', \\\n",
    "               source='',platform_id='',woceline='',profile_id=''):\n",
    "    # url_prefix: string root of API routes\n",
    "    # startDate [endDate]: string start [end] date to filter documents on, in ISO 8601 UTC datestrings ie 1999-12-31T00:00:00Z\n",
    "    # radius_km: float distance to search in proximity search; must be passed with center\n",
    "    # center: [lon, lat] list of center of proximity search; must be passed with radius_km\n",
    "    # polygon: [[lon0, lat0], [lon1, lat1], ... [lon0, lat0]] list of lists of lon/lat pairs describing polygon bounding box for region search; first coord must == last coord\n",
    "    # data: comma delimited string of data variables to seatch for ANDed together, ie 'pres,temp,doxy'. Admits negation ('pres,temp,~doxy'); will return the actual measurements listed and filter for profiles that have them. Get metadata only by including 'metadata-only'.\n",
    "    # presRange: comma delimited string indicating min and max pressure to return levels for, ie '0,100' for top 100 dbar\n",
    "    # source: comma delimited string of data sources, ANDed together, such as 'argo_core' or 'cchdo_go-ship'. Accepts negation, ie 'argo_core,~argo_bgc'\n",
    "    # platform_id: string indicating ID of Argo platform to search for\n",
    "    # woceline: string indicating WOCE line to search for\n",
    "    # profile_id: string indicating profile ID to search for\n",
    "    # returns: string URL for performing the desired search (note all filters are ANDed together).\n",
    "\n",
    "    url = url_prefix\n",
    "    \n",
    "    if startDate:\n",
    "        url = url + '&startDate=' + startDate\n",
    "    if endDate:\n",
    "        url = url + '&endDate=' + endDate\n",
    "        \n",
    "    # regional queries\n",
    "    if radius_km and center:\n",
    "        url = url + '&radius=' + radius_km + '&center=' + center\n",
    "    elif polygon:\n",
    "        url = url + '&polygon=' + polygon\n",
    "        \n",
    "    # queries by variable data\n",
    "    if data:\n",
    "        url = url + '&data=' + data\n",
    "    \n",
    "    # queries by pressure range\n",
    "    if presRange:\n",
    "        url = url + '&presRange=' + presRange\n",
    "    \n",
    "    # queries by source\n",
    "    if source:\n",
    "        url = url + '&source=' + source\n",
    "    \n",
    "    # queries by platform id\n",
    "    if platform_id:\n",
    "        url = url + '&platform_id=' + platform_id\n",
    "    \n",
    "    # queries by woceline\n",
    "    if woceline:\n",
    "        url = url + '&woceline=' + woceline\n",
    "\n",
    "    # queries by _id\n",
    "    if profile_id:\n",
    "        url = url + '&id=' + profile_id\n",
    "        \n",
    "    return url\n",
    "#####\n",
    "def get_data_for_timeRange(startDate,endDate,url_prefix, \\\n",
    "                     myAPIkey,\\\n",
    "                     radius_km=[],center=[], \\\n",
    "                     polygon=[],data='',presRange='', \\\n",
    "                     source='',platform_id='',woceline='', \\\n",
    "                     dt_tag='d',writeFlag=False):\n",
    "    # all inputs as create_url, excpet:\n",
    "    # myAPIkey: string API key for Argovis API\n",
    "    # dt_tag: frequency tag as defined at https://pandas.pydata.org/docs/user_guide/timeseries.html#timeseries-offset-aliases; \n",
    "    # determines how much data is downloaded per request (we suggest 'd' to avoid requesting too much data all at once; it should be tuned based e.g. \n",
    "    # on the size of the region of interest)\n",
    "    # returns a dataframe describing the data returned by the specified query string filters\n",
    "\n",
    "    list_of_days = create_list_of_days(startDate,endDate,dt_tag=dt_tag)\n",
    "    info_ALL = []\n",
    "    for i in np.arange(0,len(list_of_days)-1):\n",
    "        url_to_use = create_url(url_prefix=url_prefix, \\\n",
    "                               startDate=list_of_days[i], \\\n",
    "                               endDate=list_of_days[i+1], \\\n",
    "                               radius_km=radius_km,center=center, \\\n",
    "                               polygon=polygon,data=data,presRange=presRange, \\\n",
    "                               source=source,platform_id=platform_id,woceline=woceline)\n",
    "        #print(url_to_use)\n",
    "        info_ALL   = info_ALL + get_data_from_url(url=url_to_use,myAPIkey=myAPIkey,writeFlag=writeFlag)\n",
    "                               \n",
    "    info_ALL = pd.DataFrame(info_ALL)\n",
    "    \n",
    "    return info_ALL\n",
    "#####\n",
    "def get_info_from_df(df,info_to_store):\n",
    "    # df: dataframe as returned by ie get_data_for_timeRange\n",
    "    # info_to_store: list of strings indicating variables of interest\n",
    "    # returns dictionary packing of listed info from dataframe\n",
    "\n",
    "    if isinstance(df,pd.DataFrame):\n",
    "        lon  = []\n",
    "        lat  = []\n",
    "        date = []\n",
    "        cols_bySource=[]\n",
    "        ids  = []\n",
    "        woce_line = []\n",
    "        \n",
    "        lst_out = []\n",
    "        \n",
    "        for i in np.arange(0,len(df),1):\n",
    "            #\n",
    "            if any(\"lon\" in s for s in info_to_store) or  any(\"lat\" in s for s in info_to_store):\n",
    "                lon.append(df.geolocation[i]['coordinates'][0])\n",
    "                lat.append(df.geolocation[i]['coordinates'][1])\n",
    "            #\n",
    "            if any(\"date\" in s for s in info_to_store):\n",
    "                date.append(df.timestamp[i][0:-5]+'Z')\n",
    "            #\n",
    "            if any(\"ids\" in s for s in info_to_store):\n",
    "                ids.append(df._id[i])\n",
    "            #\n",
    "            if any(\"cols_bySource\" in s for s in info_to_store):\n",
    "                bfr_source= []\n",
    "                for jsource in df.source_info[i]:\n",
    "                    bfr_source = bfr_source + jsource['source']\n",
    "                cols_bySource.append(select_color_byList(lst_in=bfr_source))\n",
    "            # \n",
    "            if any(\"woce_line\" in s for s in info_to_store):\n",
    "                if \"woce_line\" in df.keys():\n",
    "                    woce_line.append(df.woce_line[i])\n",
    "                    \n",
    "        for i in info_to_store:\n",
    "            if len(eval(i)) ==len(df) or not eval(i):\n",
    "                eval('lst_out.append('+ i +')')\n",
    "            else:\n",
    "                raise Exception('check length')\n",
    "            \n",
    "        dict_info = {}\n",
    "        for i,ival in zip(info_to_store,lst_out):\n",
    "            if ival:\n",
    "                dict_info[i] = ival\n",
    "    return dict_info\n",
    "######\n",
    "# create a list of days (in string format) from string dates in input, e.g.\n",
    "#startDate='2021-05-01T00:00:00Z'\n",
    "#endDate  ='2021-05-10T00:00:00Z'\n",
    "def create_list_of_days(startDate,endDate,dt_tag='d'): \n",
    "    # dt_tag could be '30T', 'd', ...\n",
    "    list_of_days = (pd.DataFrame(columns=['NULL'],\n",
    "                            index=pd.date_range(startDate,endDate,\n",
    "                                                freq=dt_tag)) #'d'\n",
    "                                   .between_time('00:00','23:59')\n",
    "                                   .index.strftime('%Y-%m-%dT%H:%M:%SZ')\n",
    "                                   .tolist()\n",
    "                )\n",
    "    if list_of_days[-1] != endDate:\n",
    "        list_of_days.append(endDate[0:11]+'23:59:59Z')\n",
    "    return list_of_days\n",
    "\n",
    "def polygon_lon_lat(polygon_str):\n",
    "    # polygon_str: string value of polygon search parameter, ie \"[[lon0,lat0],[lon1,lat1],...,[lon0,lat0]]\"\n",
    "    # convert the polygon shape to lon and lat and save in a dictionary\n",
    "    polygon_lon_lat_dict = {'lon': [float(i) for i in ((polygon_str.replace('[','')).replace(']','')).split(',')[0::2]], \\\n",
    "                    'lat': [float(i) for i in ((polygon_str.replace('[','')).replace(']','')).split(',')[1::2]]\n",
    "                   }\n",
    "    return polygon_lon_lat_dict\n",
    "######\n",
    "########### data visualization #################\n",
    "# set up a map\n",
    "def set_up_map(set_extent_info,central_long=180,delta_lonGrid=30,delta_latGrid=30,fnt_size=28):\n",
    "    # set_extent_info: [min lon, max lon, min lat, max lat] for mapping region\n",
    "    # central_long: central longitude for map projection\n",
    "    # delta_lonGrid: how close in degrees to space longitude ticks\n",
    "    # delta_latGrid: how close in degrees to space latitude ticks\n",
    "    # fnt_size: x/y axis label font size\n",
    "\n",
    "    # this declares a recentered projection for Pacific areas\n",
    "    usemap_proj = ccrs.PlateCarree(central_longitude=central_long)\n",
    "    usemap_proj._threshold /= 20.  # to make greatcircle smooth\n",
    "\n",
    "    ax = plt.axes(projection=usemap_proj)\n",
    "    # set appropriate extents: (lon_min, lon_max, lat_min, lat_max)\n",
    "    ax.set_extent(set_extent_info, crs=ccrs.PlateCarree())\n",
    "\n",
    "    gl = ax.gridlines(draw_labels=True,color='black')\n",
    "    gl.xlabels_top = False\n",
    "    gl.ylabels_right = False\n",
    "    gl.xformatter = LONGITUDE_FORMATTER\n",
    "    gl.yformatter = LATITUDE_FORMATTER\n",
    "    gl.xlocator = ticker.FixedLocator(np.arange(-180,180,delta_lonGrid))\n",
    "    gl.ylocator = ticker.FixedLocator(np.arange(-90,90,delta_latGrid))\n",
    "\n",
    "    gl.xlabel_style = {'size': fnt_size}\n",
    "    gl.ylabel_style = {'size': fnt_size}\n",
    "\n",
    "    ax.coastlines()\n",
    "    ax.add_feature(cft.LAND)#, color='lightgray'\n",
    "    ax.add_feature(cft.OCEAN)\n",
    "    ax.add_feature(cft.COASTLINE)\n",
    "    ax.add_feature(cft.BORDERS, linestyle=':')\n",
    "\n",
    "    geodetic = ccrs.Geodetic()\n",
    "    return ax, gl, usemap_proj\n",
    "######\n",
    "def plot_locations_withColor(lon,lat,cols,markersz=10,fnt_size=28):\n",
    "    # plot a map with profile locations: if cols is only 1 string, then the same color is used for all the dots\n",
    "    # if cols is a list of strings of the same length as e.g. lon, then one col per lon is used\n",
    "    # else 'k' is used for color\n",
    "\n",
    "    for i in np.arange(0,len(lon),1):\n",
    "        if len(cols) == len(lon):\n",
    "            col = cols[i]\n",
    "        elif isinstance(col,str):\n",
    "            col = cols\n",
    "        else:\n",
    "            col = 'k'\n",
    "        plt.plot(lon[i],lat[i],marker='o',markersize=markersz,color=col,transform=ccrs.PlateCarree()) # cols_bySource[i]\n",
    "######\n",
    "def set_map_and_plot_locations_withColor(lon,lat,cols,polygon_lon_lat_dict=[],markersz=10,dx=15,dy=15,central_long=-30, \\\n",
    "                                         delta_lonGrid=15,delta_latGrid=15,fnt_size=28, \\\n",
    "                                         fig_size=(10,10)):\n",
    "    # lon: list of all longitudes of interest\n",
    "    # lat: list of all latitudes of interest\n",
    "    # polygon_lon_lat_dict: dictionary of longitudes and latitudes describing polygon region, see polygon_lon_lat function\n",
    "    # markersz: scatterplot marker size\n",
    "    # dx: degrees of margin space in longitude\n",
    "    # dy: degrees of margin space in latitude\n",
    "    # central_long, delta_lonGrid, delta_latGrid, fnt_size: see set_up_map\n",
    "    # fig_size: matplotlib figure size\n",
    "\n",
    "    # set up the map\n",
    "    fig = plt.figure(figsize=fig_size)\n",
    "    lon_all = lon\n",
    "    lat_all = lat\n",
    "    if polygon_lon_lat_dict:\n",
    "        lon_all = lon_all + polygon_lon_lat_dict['lon']\n",
    "        lat_all = lat_all + polygon_lon_lat_dict['lat']\n",
    "    ax, gl, usemap_proj = set_up_map(set_extent_info=[min(lon_all)-dx,max(lon_all)+dx,min(lat_all)-dy,max(lat_all)+dy],\n",
    "                                     central_long=central_long,\n",
    "                                     delta_lonGrid=delta_lonGrid,delta_latGrid=delta_latGrid,fnt_size=fnt_size,\n",
    "                                     )\n",
    "    # plot the locations of the porofiles and polygon\n",
    "    if polygon_lon_lat_dict:\n",
    "        plt.plot(polygon_lon_lat_dict['lon'],polygon_lon_lat_dict['lat'],'-k',transform=ccrs.PlateCarree()) \n",
    "    plot_locations_withColor(lon=lon,lat=lat, \\\n",
    "                             cols=cols,markersz=markersz,fnt_size=28)\n",
    "#######\n",
    "\n",
    "# pick color based on list of sources\n",
    "def select_color_byList(lst_in):\n",
    "    if any(\"argo_bgc\" in s for s in lst_in) and any(\"argo_deep\" in s for s in lst_in):\n",
    "        col = 'r'\n",
    "    elif any(\"argo_bgc\" in s for s in lst_in):\n",
    "        col = 'g'\n",
    "    elif any(\"argo_deep\" in s for s in lst_in):\n",
    "        col = 'b'\n",
    "    elif any(\"argo_core\" in s for s in lst_in):\n",
    "        col = 'y'\n",
    "    elif any(\"cchdo_go\" in s for s in lst_in):\n",
    "        col = 'k'\n",
    "    else:\n",
    "        col = 'gray'\n",
    "    return col   \n",
    "######    \n",
    "# pick label\n",
    "def set_ax_label(str_in):\n",
    "    if str_in=='psal':\n",
    "        ax_lab = 'Salinity [psu]'\n",
    "    elif str_in=='temp':\n",
    "        ax_lab = 'Temperature [degC]'\n",
    "    elif str_in=='pres':\n",
    "        ax_lab = 'Pressure [dbar]'\n",
    "    elif str_in=='doxy':\n",
    "        ax_lab = 'Oxygen, micromole/kg'\n",
    "    else:\n",
    "        ax_lab = str_in\n",
    "    return ax_lab\n",
    "#######\n",
    "def qc_suffix(profile):\n",
    "    # given a <profile> object \n",
    "    # return what the corresponding QC variable suffix.\n",
    "\n",
    "    qcsuffix = None\n",
    "    if 'argo' in profile['source_info'][0]['source'][0]:\n",
    "        qcsuffix = '_argoqc'\n",
    "    else:\n",
    "        qcsuffix = '_woceqc'\n",
    "\n",
    "    return qcsuffix    \n",
    "#######\n",
    "def qc(profile, qc_levels=[]):\n",
    "    # given a <profile> and a list <qc_levels> of tuples (<variable>, <[allowed qcs]>),\n",
    "    # Suppress all measurements that don't pass the specified QC.\n",
    "    # If any measurements have qc present in <profiles.data> but don't have a level set in <qc_levels>,\n",
    "    # require QC==1 for argo data and ==2 for CCHDO.\n",
    "\n",
    "    qcsuffix = qc_suffix(profile)        \n",
    "    qclookup = dict(qc_levels)\n",
    "    \n",
    "    if 'data' in profile and 'data_keys' in profile:\n",
    "        for k in profile['data_keys']:\n",
    "            if qcsuffix not in k: \n",
    "                # insist all data comes with qc\n",
    "                if k+qcsuffix not in profile['data_keys']:\n",
    "                    print(k, 'present without its QC; please add', k+qcsuffix, 'to your data query.')\n",
    "                    return None\n",
    "                if k in qclookup:\n",
    "                    profile = mask_QC(profile, k, qclookup[k])\n",
    "                elif qcsuffix == '_argoqc':\n",
    "                    profile = mask_QC(profile, k, [1]) # default QC==1 for Argo \n",
    "                elif qcsuffix == '_woceqc':\n",
    "                    profile = mask_QC(profile, k, [2]) # default QC==1,2 for CCHDO\n",
    "    return profile\n",
    "######            \n",
    "def mask_QC(profile, variable, allowed_qc):\n",
    "    # given a <profile> object, set <variable> to None if its QC flag is not in the list <allowed_qc>, \n",
    "    # and return the resulting profile object\n",
    "\n",
    "    qcsuffix = qc_suffix(profile)  \n",
    "\n",
    "    # helper for masking a single level dict; missing QC info == failed\n",
    "    def m(level, var,qc,allowed_qc):\n",
    "        if not level[qc] or level[qc] not in allowed_qc:\n",
    "            level[var] = None\n",
    "        return level\n",
    "\n",
    "\n",
    "    masked_profile = copy.deepcopy(profile) # don't mutate the original\n",
    "    if 'data' in masked_profile and variable in masked_profile['data_keys'] and variable+qcsuffix in masked_profile['data_keys']:\n",
    "        masked_profile['data'] = [m(level,variable,variable+qcsuffix,allowed_qc) for level in masked_profile['data']]\n",
    "\n",
    "    return masked_profile\n",
    "######\n",
    "def simple_plot(profile, variable, variable_qc=None):\n",
    "\n",
    "    if 'data' in profile and variable in profile['data_keys']:\n",
    "        if variable_qc and variable_qc in profile['data_keys']:\n",
    "            plt.scatter([x[variable] for x in profile['data']],[y['pres'] for y in profile['data']], c=[c[variable_qc] for c in profile['data']])\n",
    "            plt.colorbar()\n",
    "        else:\n",
    "            plt.scatter([x[variable] for x in profile['data']],[y['pres'] for y in profile['data']])\n",
    "    \n",
    "    plt.xlabel(variable)\n",
    "    plt.ylabel('pres')\n",
    "    plt.gca().invert_yaxis()\n",
    "######\n",
    "def interpolate(profile, levels, method='pchip'):\n",
    "    # given a <profile> and a list of desired pressure <levels>,\n",
    "    # return a profile with profile.data levels at the desired pressure levels, with all available data interpolated to match\n",
    "    # drop all QC and note `data_interpolated` in profile.data_warnings\n",
    "\n",
    "    if method not in ['pchip', 'linear', 'nearest']:\n",
    "        print('method must be one of pchip, linear or nearest')\n",
    "        return None\n",
    "    \n",
    "    data_names = ['pres']\n",
    "    interpolated_data = [levels]\n",
    "    for key in profile['data_keys']:\n",
    "        if '_argoqc' not in key and '_woceqc' not in key and key!='pres':\n",
    "            finites = [(level['pres'], level[key]) for level in profile['data'] if level['pres'] is not None and level[key] is not None and not math.isnan(level['pres']) and not math.isnan(level[key])]\n",
    "            pres = [x[0] for x in finites]\n",
    "            data = [x[1] for x in finites]\n",
    "            data_names.append(key)\n",
    "            if method == 'pchip':\n",
    "                interpolated_data.append(scipy.interpolate.pchip_interpolate(pres, data, levels))\n",
    "            elif method == 'linear':\n",
    "                f = scipy.interpolate.interp1d(pres, data, kind='linear', fill_value='extrapolate')\n",
    "                interpolated_data.append([f(x) for x in levels])\n",
    "            elif method == 'nearest':\n",
    "                f = scipy.interpolate.interp1d(pres, data, kind='nearest', fill_value='extrapolate')\n",
    "                interpolated_data.append([f(x) for x in levels])\n",
    "    \n",
    "    interpolated_levels = list(zip(*interpolated_data))\n",
    "    data = [{data_names[i]:d[i] for i in range(len(data_names))} for d in interpolated_levels]\n",
    "    interpolated_profile = copy.deepcopy(profile) # don't mutate the original\n",
    "    interpolated_profile['data'] = data\n",
    "    if 'data_warnings' in interpolated_profile:\n",
    "        interpolated_profile['data_warnings'].append('data_interpolated')\n",
    "    else:\n",
    "        interpolated_profile['data_warnings'] = ['data_interpolated']\n",
    "    return interpolated_profile\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "argovis_demos",
   "language": "python",
   "name": "argovis_demos"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
